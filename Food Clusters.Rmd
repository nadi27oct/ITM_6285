---
title: "Product Clusters Based on Nutritional Content"
author: "Imali Nadya Wanigasundara"
output:
  html_notebook: default
  pdf_document: default
---

This document uses various clustering methods to categorize food clusters based on nutritional content. 
The dataset on food products is taken from openfoodfacts.org.


####Cleaning the Dataset

First, read the data and specify variable characters as numeric and character. Here the first two columns, product ID and product names are characters while rest of the columns that contain nutritional contents is numeric.
Then create a subset of data that contains rows with less than 25 undefined values and set the product name to be the rows of the new dataset, CleanPoductData.
Finally scale the dataset so that clustering algorithm will weigh each variable evenly.

```{r}
# Read the data 
ProductData <- read.csv("~/Documents/homework/ITM_6285/products2.csv", na.strings = "undefined",
                      colClasses = c("character", "character", rep("numeric", 48)))
# Delete the first column
ProductData$prodid <-NULL
# Selecting a subset with less than 25 undefined values
ProdDataSubset <- ProductData[rowSums(is.na(ProductData))<25,]
summary(ProdDataSubset)
# Remove duplicates
CleanProductData <- ProdDataSubset %>% distinct(prodname, .keep_all = TRUE)
# Set undefined values to 0.
CleanProductData[is.na(CleanProductData)] <- 0
# Set productname as row names
rownames(CleanProductData) <- CleanProductData[,1]
CleanProductData$prodname <- NULL
# Scale the data
ScaledProdData <- scale(CleanProductData)
ScaledProdData[is.na(ScaledProdData)] <- 0
```

*Note: Refer to appendix A for libraries used for clustering.*

### 1) Hierarchical Clustering

First, create a distance matrix based on Euclidean distance.
This distance matrix is visually represented by the dendrogram below where the objects that are similar are closest to each other and objects that are different are furthest apart. 

```{r, fig.height=11}
prodhieclusters <- agnes(ScaledProdData, method = "complete", metric = "euclidean")
plot(prodhieclusters, which.plots=2, cex = 1.0)
```

Based on the dendrogram, we can observe that similar products are clustered together. For instance, when we cut the tree at about height level 5, we can see that Fruit Loops, Special K Red Berries and Special K Protein are close relatives. Similarly, sliced almonds and lightly saled cashews share the same branch. It is also interesting to observe that apricot is closer to chocolate chip and macademia nut.


### 2) K-Means Clustering

The k-means algorithm creates product clusters based on the best centroids for each group. Based on those centroids, the algorithm determine which products belong together based on their nutrients.

Assume we want to create 5 clusters. The k-means algorithm for 5 clusters shows that total distance within each centroid and each cluster for all 5 clusters is 793 while the total distance among 5 centroids is 803.
```{r}
prodkclusters <- kmeans(ScaledProdData, 5, nstart = 25)
prodkclusters$withinss
prodkclusters$tot.withinss
prodkclusters$betweenss
```

Since the product dataset contains 48 variables, it is difficult to graphically represent the datasets in all clusters as plots are constrained to 2 dimensional graphs.

Here we assume that k= 5 is the ideal number of clusters, however, since the dataset contains several variables, we may underestimate this number. The NBClust algorithm can be used here to determine the best number of clusters as below. Here we are using the silhouette statistic to compute the optimal clusters, specifying 5 as the minimum number and 15 to be the maximum clusters.

Per NbClust algorithm, maximum clusters is identified to be 7.

```{r}
bestK<- NbClust(data = ScaledProdData, diss = NULL, distance = "euclidean", min.nc = 5, 
                max.nc = 15, method = "kmeans", index = "silhouette", alphaBeale = 0.1)
bestK$Best.nc
```

### 3) Kohonen SOM (Self Organizing Maps)

Kohonen SOM use nodes vectors to represent clusters. The som algorithm can be visually represented using som map function as below. We specify the map to contain a 5 * 4 node matrix. Reducing the matrix size is likely to cluster products with dissimilarities together. 

We can observe that Oat Bran and Nut Bran are closer to node with Grape Nuts Cereral and Frosted Flakes.

```{r, fig.height=8, fig.width=10}
kohsom <- som(data=ScaledProdData, grid = somgrid(5, 4, "hexagonal"))
plot(kohsom, type="mapping", labels = rownames(ScaledProdData), cex=0.6)
```

Similarly, based on the visualization of weight vectors, we can observe that nodes with similar signatures are placed together.
For instance, nodes on the left side of the plot contains several spikes while nodes on the bottom right side contains fairly stable signatures.


```{r, fig.height=6}
plot(kohsom, type="code", labels = rownames(ScaledProdData))

```

### 4) HeatMap with Hierarchical Cluster

Below is a heatmap combined with a dendogram to visualize the hierarchical structure of clusters organized in a heatmap. Cells colored in yellow and light green show a very high value of a nutrition. For instance, Grape Nut cereal has a high value of insoluble fiber. Vanilla Soy Milk and Sweetened Soy Milk has low levels of vitamin pp and vitamin b1 serving, so these two items are clustered together. Similarly, Tootie Fruities and Lucky Charms, two colored cereals, are in the same cluster and they contain the same amount of zinc serving.

```{r, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
heatmaply(ScaledProdData, k_col = 2, k_row = 3) %>% layout(margin = list(l = 140, b = 140))
```

Below plot displays the multi-dimensional food dataset with a heatmap without the dendrogram.
Here we can observe that 

```{r, fig.height=7, fig.width=7}
product_heatmap <- heatmap(ScaledProdData, Rowv=NA, Colv=NA, col = heat.colors(256), scale="column", margins=c(10,10))
```


####Appendix A

Load all relevant libraries.

```{r}
library(readr)
library(cluster)
library(ggplot2)
library(dendextend)
library(dplyr)
library(NbClust)
library(kohonen)
library(class)
library(MASS)
library(heatmaply)
```






